{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f3fdba-49a5-4a88-bafb-7b3159f7b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import wrf\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# import personal modules\n",
    "# Path to modules\n",
    "sys.path.append('../modules')\n",
    "# Import my modules\n",
    "from utils import roundPartial, generate_ptlst_from_start_end\n",
    "from plotter import draw_basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f91bf34-6050-408c-9934-1418110854e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '/expanse/nfs/cw3e/cwp140/'\n",
    "path_to_out  = '../out/'       # output files (numerical results, intermediate datafiles) -- read & write\n",
    "path_to_figs = '../figs/'      # figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e785ad-a0d4-4b9b-b300-0ae61f04cbf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yaml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# import configuration file for case study choice\u001b[39;00m\n\u001b[1;32m      3\u001b[0m yaml_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomains.yml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(yaml_doc), Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mSafeLoader)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#### FOR BBOX METHOD ####                  \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m## create a dataset with lats and lons from ext\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ext \u001b[38;5;241m=\u001b[39m d[region][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yaml' is not defined"
     ]
    }
   ],
   "source": [
    "region = 'gulf_of_mexico' ## 'san_juan' 'baja' 'gulf_of_mexico'\n",
    "# import configuration file for case study choice\n",
    "yaml_doc = 'domains.yml'\n",
    "d = yaml.load(open(yaml_doc), Loader=yaml.SafeLoader)\n",
    "\n",
    "#### FOR BBOX METHOD ####                  \n",
    "## create a dataset with lats and lons from ext\n",
    "ext = d[region]['ext']\n",
    "lats = np.arange(ext[2], ext[3]+0.25, 0.25)\n",
    "lons = np.arange(ext[0], ext[1]+0.25, 0.25)\n",
    "\n",
    "print(lats)\n",
    "print(lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba04ff6-0ce3-47a8-ad98-9a668864d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## create a simple map to show the areas\n",
    "nrows = 1\n",
    "ncols = 1\n",
    "mapcrs = ccrs.PlateCarree()\n",
    "datacrs = ccrs.PlateCarree()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "## Use gridspec to set up a plot with a series of subplots that is\n",
    "## n-rows by n-columns\n",
    "gs = GridSpec(nrows, ncols, height_ratios=[1], width_ratios = [1], wspace=0.025, hspace=0.05)\n",
    "## use gs[rows index, columns index] to access grids\n",
    "\n",
    "## add basemap\n",
    "ext = [-140., -90., 20, 50]\n",
    "dx = np.arange(ext[0],ext[1]+10,10)\n",
    "dy = np.arange(ext[2],ext[3]+10,10)\n",
    "ax = fig.add_subplot(gs[0, 0], projection=mapcrs)\n",
    "ax = draw_basemap(ax, extent=ext, xticks=dx, yticks=dy, grid= True, left_lats=True, right_lats=False)\n",
    "ax.add_feature(cfeature.STATES, edgecolor='0.4', linewidth=0.8)\n",
    "ax.set_extent(ext, datacrs)\n",
    "\n",
    "for i, domain in enumerate(['san_juan', 'baja', 'gulf_of_mexico']):\n",
    "    ext3 = d[domain]['ext']\n",
    "    ax.add_patch(mpatches.Rectangle(xy=[ext3[0], ext3[2]], width=ext3[1]-ext3[0], height=ext3[3]-ext3[2],\n",
    "                                            fill=False,\n",
    "                                            edgecolor='r',\n",
    "                                            linewidth=0.75,\n",
    "                                            transform=datacrs,\n",
    "                                            zorder=199))\n",
    "\n",
    "    ax.plot([d[domain]['start_pt'][1], d[domain]['end_pt'][1]],\n",
    "            [d[domain]['start_pt'][0], d[domain]['end_pt'][0]], color=\"blue\",\n",
    "                transform=ccrs.PlateCarree(), zorder=3)\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05b641-d2c3-4447-97b1-3dd7fce3d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load PRISM watershed precip dataset\n",
    "fname = path_to_data + 'preprocessed/PRISM/PRISM_HUC8_CO_sp.nc'\n",
    "PRISM = xr.open_dataset(fname)\n",
    "HUC8_lst = PRISM.HUC8.values ## get list of HUC8 IDs\n",
    "\n",
    "ds_lst = []\n",
    "## load final trajectory dataset\n",
    "for i, HUC8_ID in enumerate(HUC8_lst):\n",
    "    fname = path_to_data + 'preprocessed/ERA5_trajectories/combined_extreme_AR/PRISM_HUC8_{0}.nc'.format(HUC8_ID)\n",
    "    traj = xr.open_dataset(fname)\n",
    "    ds_lst.append(traj)\n",
    "\n",
    "## concat ds_lst along HUC8 index\n",
    "ds = xr.concat(ds_lst, pd.Index(HUC8_lst, name=\"HUC8\"))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d82add-7901-43ed-8cf2-67ccf9d5ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "    \n",
    "def find_time_bbox(ERA5, lats, lons):\n",
    "    ## a function that gets the start dates and HUC8 ID \n",
    "    ## for the times when a trajectory is within the bbox\n",
    "    \n",
    "    ## create a dataset of the trajectory points that match ERA5 spacing\n",
    "    t = xr.DataArray(ERA5.time.values, dims=['location'], name='time')\n",
    "    \n",
    "    # create a list of lat/lons that match ERA5 spacing\n",
    "    x = xr.DataArray(roundPartial(ERA5.lon.values, 0.25), dims=['location'])\n",
    "    y = xr.DataArray(roundPartial(ERA5.lat.values, 0.25), dims=['location'])\n",
    "    \n",
    "    x = xr.DataArray(ERA5.lon.values, dims=(\"location\"), coords={\"lon\": x}, name='traj_lons')\n",
    "    y = xr.DataArray(ERA5.lat.values, dims=(\"location\"), coords={\"lat\": y}, name='traj_lats')\n",
    "    \n",
    "    # create a new dataset that has the trajectory lat and lons and the closest ERA5 lat/lons as coords\n",
    "    z = xr.merge([x, y, t])\n",
    "    \n",
    "    ## Now loop through the lat/lon pairs and see where they match\n",
    "    idx_lst = []\n",
    "    for i, (x, y) in enumerate(zip(z.lon.values, z.lat.values)):\n",
    "        for j, lon in enumerate(lons):\n",
    "            for k, lat in enumerate(lats):\n",
    "            \n",
    "                ## test if lat/lon pair matches\n",
    "                result_variable = (lon == x) & (lat == y)\n",
    "        \n",
    "                if (result_variable == True):\n",
    "                    idx = (i, j, k) # (index of z, index of txtpts)\n",
    "                    idx_lst.append(idx)\n",
    "    ts_lst = []\n",
    "    if len(idx_lst) > 0:\n",
    "        for m, idx in enumerate(idx_lst):\n",
    "            ## this is the time of the trajectory when it crosses west coast\n",
    "            time_match = z.sel(location=idx_lst[m][0]).time.values\n",
    "            ts = pd.to_datetime(str(time_match)).strftime('%Y-%m-%d %H')\n",
    "            ts_lst.append(ts)\n",
    "\n",
    "    return ts_lst\n",
    "\n",
    "def find_time_line(ERA5, lats, lons):\n",
    "    ## create a dataset of the trajectory points that match ERA5 spacing\n",
    "    t = xr.DataArray(ERA5.time.values, dims=['location'], name='time')\n",
    "    lev = xr.DataArray(ERA5.level.values, dims=['location'], name='level')\n",
    "    \n",
    "    # create a list of lat/lons that match ERA5 spacing\n",
    "    x = xr.DataArray(roundPartial(ERA5.lon.values, 0.25), dims=['location'])\n",
    "    y = xr.DataArray(roundPartial(ERA5.lat.values, 0.25), dims=['location'])\n",
    "    \n",
    "    x = xr.DataArray(ERA5.lon.values, dims=(\"location\"), coords={\"lon\": x}, name='traj_lons')\n",
    "    y = xr.DataArray(ERA5.lat.values, dims=(\"location\"), coords={\"lat\": y}, name='traj_lats')\n",
    "    \n",
    "    # create a new dataset that has the trajectory lat and lons and the closest ERA5 lat/lons as coords\n",
    "    z = xr.merge([x, y, t, lev])\n",
    "    \n",
    "    idx_lst = []\n",
    "    for i, (x, y) in enumerate(zip(z.lon.values, z.lat.values)):\n",
    "        for j, pair in enumerate(coord_pairs):\n",
    "            lat, lon = pair\n",
    "            ## test if lat/lon pair matches\n",
    "            result_variable = (lon == x) & (lat == y)\n",
    "    \n",
    "            if (result_variable == True):\n",
    "                idx = (i, j) # (index of z, index of txtpts)\n",
    "                idx_lst.append(idx)\n",
    "    \n",
    "    ts_lst = []\n",
    "    if len(idx_lst) > 0:\n",
    "        for m, idx in enumerate(idx_lst):\n",
    "            ## this is the time of the trajectory when it crosses the transect\n",
    "            time_match = z.sel(location=idx_lst[m][0])\n",
    "            ts_lst.append(time_match) ## append the entire xr dataset\n",
    "    \n",
    "    return ts_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd5b34-a741-4228-ae3b-62aa2ee720ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FOR CROSS SECTION METHOD ####\n",
    "coord_pairs = generate_ptlst_from_start_end(d[region]['start_pt'][1], d[region]['start_pt'][0], d[region]['end_pt'][1], d[region]['end_pt'][0], pairs=True)\n",
    "print(coord_pairs)\n",
    "df_lst = []\n",
    "HUC8_final = []\n",
    "## loop through HUC8s and start_dates\n",
    "for i, HUC8 in enumerate(HUC8_lst):\n",
    "    # subset to the current HUC8\n",
    "    ## keep only trajectories associated with ARs\n",
    "    tmp = ds.sel(HUC8=HUC8)\n",
    "    tmp = tmp.where(tmp.tARget > 0, drop=True)\n",
    "\n",
    "\n",
    "    t_lst = []\n",
    "    ## enumerate through start_dates of current subbasin\n",
    "    for i, st_date in enumerate(tmp.start_date.values):\n",
    "        ERA5 = tmp.sel(start_date=st_date)\n",
    "        time_lst = find_time_line(ERA5, lats, lons)\n",
    "        t_lst.append(time_lst)\n",
    "    \n",
    "    t_lst = flatten(t_lst)\n",
    "    if len(t_lst) == 0:\n",
    "        pass\n",
    "    elif len(t_lst) == 1:\n",
    "        HUC8_final.append(HUC8)\n",
    "        df_full = t_lst[0].expand_dims(dim={\"time\": 1}).to_dataframe()\n",
    "        df_full['HUC8'] = HUC8\n",
    "        df_lst.append(df_full)\n",
    "    elif len(t_lst) > 1:\n",
    "        HUC8_final.append(HUC8)\n",
    "        df_full = xr.concat(t_lst, dim='time').to_dataframe()\n",
    "        df_full['HUC8'] = HUC8\n",
    "        df_lst.append(df_full)\n",
    "\n",
    "## now we have a list of list of dates when AR trajectories were within bbox\n",
    "## concat df_lst\n",
    "df = pd.concat(df_lst)\n",
    "\n",
    "## create a column with the coord pair\n",
    "df['coord_pair'] = list(zip(df.lat, df.lon))\n",
    "\n",
    "## save as CSV dates_region-name.csv\n",
    "fname_out = '../out/line_dates_{0}_full.csv'.format(region)\n",
    "df.to_csv(fname_out)\n",
    "\n",
    "## make a copy of the df but keep only time/index\n",
    "d = {'datetime': df.index.values}\n",
    "times_df = pd.DataFrame(d)\n",
    "times_df = times_df.drop_duplicates(subset=['datetime'])\n",
    "\n",
    "## save as CSV dates_region-name.csv\n",
    "fname_out = '../out/line_dates_{0}_tARget.csv'.format(region)\n",
    "times_df.to_csv(fname_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c986b-5c37-4f48-888b-52ed91b29901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61e23a-1cc5-48a0-be80-fa4121b1b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5578ca-3de7-4a78-919b-fde028f2fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "x = df.lat.values\n",
    "y = df.level.values\n",
    "\n",
    "# Draw a combo histogram and scatterplot with density contours\n",
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.scatterplot(x=x, y=y, s=5, color=\".15\")\n",
    "# sns.histplot(x=x, y=y, bins=50, pthresh=.1, cmap=\"mako\")\n",
    "sns.kdeplot(x=x, y=y, levels=5, color=\"k\", linewidths=1)\n",
    "plt.gca().invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb47c2-0678-432e-908b-d2e586eb0239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.interpolate import griddata\n",
    "\n",
    "# # Create a grid for interpolation\n",
    "# xi = np.arange(df.lat.values.min(), df.lat.values.max()+0.25, 0.25)\n",
    "# yi = np.linspace(200, 1000, 25)\n",
    "# xi, yi = np.meshgrid(xi, yi)\n",
    "# z = np.sin(x * 2 * np.pi) + np.cos(y * 2 * np.pi)\n",
    "\n",
    "# z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14f400-61d4-432b-b67d-8478a53677e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = np.ones(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d6e44-bd24-401b-829e-4a4851b0eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Interpolate the data onto the grid\n",
    "# zi = griddata((x, y), z, (xi, yi), method='cubic')\n",
    "\n",
    "# # Create the contour plot\n",
    "# plt.contourf(xi, yi, zi, cmap='viridis')\n",
    "# plt.colorbar()\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.gca().invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f34d8f-48aa-44c5-87b4-4c0edd8a052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lst = []\n",
    "## loop through HUC8s and start_dates\n",
    "for i, HUC8 in enumerate(HUC8_lst):\n",
    "    # subset to the current HUC8\n",
    "    ## keep only trajectories associated with ARs\n",
    "    tmp = ds.sel(HUC8=HUC8)\n",
    "    tmp = tmp.where(tmp.tARget > 0, drop=True)\n",
    "\n",
    "\n",
    "    t_lst = []\n",
    "    ## enumerate through start_dates of current subbasin\n",
    "    for i, st_date in enumerate(tmp.start_date.values):\n",
    "        ERA5 = tmp.sel(start_date=st_date)\n",
    "        test = find_time_bbox(ERA5, lats, lons)\n",
    "        t_lst.append(test)\n",
    "    \n",
    "    t_lst = flatten(t_lst)\n",
    "    df = pd.DataFrame(data={'date': t_lst}) # put dates into a df\n",
    "    # t_lst = pd.to_datetime(t_lst, format='%Y-%m-%d %H')\n",
    "    df_lst.append(df)\n",
    "\n",
    "## now we have a list of list of dates when AR trajectories were within bbox\n",
    "## concat df_lst\n",
    "df = pd.concat(df_lst)\n",
    "## swap to datetime\n",
    "df['datetime'] = pd.to_datetime(df['date'])\n",
    "\n",
    "## remove duplicates\n",
    "df = df.drop_duplicates(subset=['datetime'])\n",
    "df = df.drop(['date'], axis=1)\n",
    "\n",
    "## save as CSV dates_region-name.csv\n",
    "fname_out = '../out/bbox_dates_{0}_tARget.csv'.format(region)\n",
    "df.to_csv(fname_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbaa575-754d-44d6-aad8-019c6067f4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ade24e-98bc-4dbf-a428-d76c0582163a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SEAK-WRF]",
   "language": "python",
   "name": "conda-env-SEAK-WRF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
